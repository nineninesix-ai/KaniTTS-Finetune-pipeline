{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üòª Kani TTS - Fast and Expressive Speech Generation Model\n",
        "\n",
        "[![](https://dcbadge.limes.pink/api/server/https://discord.gg/4fZ4mjD3)](https://discord.gg/4fZ4mjD3)\n",
        "\n",
        "### Welcome to Kani TTS Fine-Tuning! Here you can adapt our breakthrough neural text-to-speech model to your own speaker, creating a personalized voice with the same speed and quality of generation."
      ],
      "metadata": {
        "id": "947Q5ElbBDcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.nineninesix.ai/kitty.png\" width=\"300\">"
      ],
      "metadata": {
        "id": "Ca2VDa4gBMwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing dependencies\n",
        "\n",
        "logo = \"\"\"\n",
        "===============================================\n",
        "          N I N E N I N E S I X  üòº\n",
        "===============================================\n",
        "\n",
        "          /\\\\_/\\\\\n",
        "         ( -.- )‚îÄ‚îÄ‚îÄ‚îê\n",
        "          > ^ <    ‚îÇ\n",
        "===============================================\n",
        "\n",
        "\"\"\"\n",
        "print(logo)\n",
        "\n",
        "!pip install transformers==4.54.0 trl>=0.18.2 peft>=0.15.2\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "import trl\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, List\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ü§ó Transformers version: {transformers.__version__}\")\n",
        "print(f\"üìä TRL version: {trl.__version__}\")"
      ],
      "metadata": {
        "id": "3FIcp_wo9nsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "35741e87-f8c5-4914-c71b-bfe19221deaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "          N I N E N I N E S I X  üòº\n",
            "===============================================\n",
            "\n",
            "          /\\_/\\\n",
            "         ( -.- )‚îÄ‚îÄ‚îÄ‚îê\n",
            "          > ^ <    ‚îÇ\n",
            "===============================================\n",
            "\n",
            "\n",
            "üì¶ PyTorch version: 2.8.0+cu126\n",
            "ü§ó Transformers version: 4.54.0\n",
            "üìä TRL version: 0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Login to HuggingFace"
      ],
      "metadata": {
        "id": "MgJw-v2W0-sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store\n",
        "!hf auth login"
      ],
      "metadata": {
        "id": "enKFavni1J05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR BASE MODEL"
      ],
      "metadata": {
        "id": "1n4x5F3vDnqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"nineninesix/kani-tts-450m-0.2-pt\""
      ],
      "metadata": {
        "id": "bSFy5le13TN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nano Dataset\n",
        "\n",
        "The following section provides functionality for preprocessing your dataset, which is stored in the form of audio tokens produced by the NVIDIA Nano codec.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "‚ÄºÔ∏è This [repository](https://github.com/nineninesix-ai/nano-codec-dataset-pipeline) will help you prepare your own dataset by tokenizing it using Nano Codec.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bQcdXn3_zdZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make Dataset Config\n",
        "\n",
        "\"\"\"\n",
        "Multi-Speaker TTS Dataset Configuration\n",
        "========================================\n",
        "\n",
        "This configuration system enables flexible dataset construction for training Text-to-Speech\n",
        "models with optional multi-speaker support. It allows you to combine multiple Hugging Face\n",
        "datasets, where each dataset can represent a different speaker or voice characteristic.\n",
        "\n",
        "Core Concepts\n",
        "-------------\n",
        "\n",
        "1. **Multi-Speaker Training**:\n",
        "   Each HFDataset can be configured with a unique `speaker_id` that gets prepended to every\n",
        "   text example before tokenization. For example, if speaker_id=\"alice\", the training sample\n",
        "   \"Hello world\" becomes \"alice: Hello world\". This conditions the model to generate speech\n",
        "   in that speaker's voice.\n",
        "\n",
        "2. **Single-Speaker Training**:\n",
        "   Simply omit the `speaker_id` parameter in all dataset definitions. The model will train\n",
        "   without speaker conditioning, learning a single voice from all combined data.\n",
        "\n",
        "3. **Dataset Merging**:\n",
        "   Multiple datasets (HFDataset1, HFDataset2, HFDataset3, etc.) are automatically merged\n",
        "   into one unified training dataset. This enables combining different speakers, languages,\n",
        "   or recording conditions in a single model.\n",
        "\n",
        "Configuration Components\n",
        "------------------------\n",
        "\n",
        "**CategoricalFilter** (Optional):\n",
        "    Filters a dataset to include only specific samples based on a column value.\n",
        "    Common use case: Extracting one speaker from a multi-speaker dataset.\n",
        "\n",
        "    Example:\n",
        "        CategoricalFilter(column_name=\"speaker\", value=\"ex02\")\n",
        "        # Keeps only samples where speaker==\"ex02\"\n",
        "\n",
        "**HFDatasetN** (Dataset Definition):\n",
        "    Defines a single dataset source with the following key parameters:\n",
        "\n",
        "    - reponame: Hugging Face dataset repository identifier\n",
        "    - split: Dataset split to use (\"train\", \"test\", \"validation\")\n",
        "    - text_col_name: Column containing the text transcriptions\n",
        "    - nano_layer_1/2/3/4: Name of columns with audio codec token sequences (4 layers).\n",
        "      If these columns will be named differently in your dataset, please indicate their\n",
        "      names so that the names are brought to a common standard.\n",
        "    - encoded_len: Name of column containing audio duration metadata\n",
        "    - speaker_id: (Optional) Voice identity prefix added to all text samples\n",
        "    - max_len: (Optional) Random sampling limit to prevent data imbalance\n",
        "    - categorical_filter: (Optional) Filter to select subset of samples\n",
        "\n",
        "**Config** (Top-Level):\n",
        "    - max_duration_sec: Global filter - excludes all samples longer than this (in seconds)\n",
        "    - hf_datasets: List of HFDataset definitions to merge\n",
        "\n",
        "Key Features\n",
        "------------\n",
        "\n",
        "1. **Speaker Identity Control**:\n",
        "   The `speaker_id` parameter is NOT a column name - it's the actual speaker identifier\n",
        "   that will be injected into training examples:\n",
        "\n",
        "   ```python\n",
        "   # Configuration\n",
        "   speaker_id = \"simon\"\n",
        "\n",
        "   # Original text\n",
        "   \"How are you today?\"\n",
        "\n",
        "   # Becomes (before tokenization)\n",
        "   \"simon: How are you today?\"\n",
        "   ```\n",
        "\n",
        "2. **Dataset Balancing with max_len**:\n",
        "   When datasets have vastly different sizes, use `max_len` to randomly sample a fixed\n",
        "   number of examples. This prevents larger datasets from dominating training.\n",
        "\n",
        "   ```python\n",
        "   # Dataset with 50,000 samples - limit to 2,000\n",
        "   HFDataset2(\n",
        "       reponame=\"...\",\n",
        "       speaker_id=\"puck\",\n",
        "       max_len=2000  # Randomly selects 2000 samples\n",
        "   )\n",
        "   ```\n",
        "\n",
        "   Omit `max_len` to include all available samples.\n",
        "\n",
        "3. **Speaker Filtering**:\n",
        "   Use `categorical_filter` to extract a single speaker from multi-speaker datasets:\n",
        "\n",
        "   ```python\n",
        "   HFDataset1(\n",
        "       reponame=\"multi_speaker_dataset\",\n",
        "       speaker_id=\"simon\",\n",
        "       categorical_filter=CategoricalFilter(\n",
        "           column_name=\"speaker\",\n",
        "           value=\"ex02\"  # Keep only this speaker from source dataset\n",
        "       )\n",
        "   )\n",
        "   ```\n",
        "\n",
        "4. **Duration Filtering**:\n",
        "   The global `max_duration_sec` parameter ensures no training sample exceeds a certain\n",
        "   audio length. This is crucial for:\n",
        "   - Memory management (long sequences need more GPU RAM)\n",
        "   - Training stability (very long sequences can cause gradient issues)\n",
        "   - Consistent batch processing\n",
        "\n",
        "Workflow\n",
        "--------\n",
        "\n",
        "1. **Define Datasets**: Create HFDataset1, HFDataset2, etc. with your sources\n",
        "2. **Configure Speakers**: Add `speaker_id` for multi-speaker training (or omit for single)\n",
        "3. **Balance Data**: Use `max_len` if needed to equalize dataset contributions\n",
        "4. **Filter Samples**: Apply `categorical_filter` to select specific speakers\n",
        "5. **Set Duration Limit**: Configure `max_duration_sec` for your GPU memory\n",
        "6. **Build Config**: Collect all datasets in Config.hf_datasets list\n",
        "7. **Run Merge Script**: The system combines everything into one training dataset\n",
        "\n",
        "Example Configurations\n",
        "----------------------\n",
        "\n",
        "**Multi-Speaker Setup**:\n",
        "```python\n",
        "Config(\n",
        "    max_duration_sec=12,\n",
        "    hf_datasets=[\n",
        "        HFDataset1(reponame=\"...\", speaker_id=\"simon\", max_len=2000),\n",
        "        HFDataset2(reponame=\"...\", speaker_id=\"puck\", max_len=2000),\n",
        "        HFDataset3(reponame=\"...\", speaker_id=\"kore\", max_len=2000),\n",
        "    ]\n",
        ")\n",
        "# Model learns 3 distinct voices with balanced data\n",
        "```\n",
        "\n",
        "**Single-Speaker Setup**:\n",
        "```python\n",
        "Config(\n",
        "    max_duration_sec=15,\n",
        "    hf_datasets=[\n",
        "        HFDataset1(reponame=\"...\", categorical_filter=CategoricalFilter(...)),\n",
        "        HFDataset2(reponame=\"...\"),\n",
        "        # No speaker_id specified - single voice training\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "**Filtered Multi-Speaker**:\n",
        "```python\n",
        "Config(\n",
        "    max_duration_sec=10,\n",
        "    hf_datasets=[\n",
        "        HFDataset1(\n",
        "            reponame=\"large_multi_speaker_corpus\",\n",
        "            speaker_id=\"alice\",\n",
        "            categorical_filter=CategoricalFilter(column_name=\"speaker\", value=\"spk_001\"),\n",
        "            max_len=3000\n",
        "        ),\n",
        "        HFDataset2(\n",
        "            reponame=\"small_single_speaker\",\n",
        "            speaker_id=\"bob\"\n",
        "            # No max_len - use all samples\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "Technical Notes\n",
        "---------------\n",
        "\n",
        "- All audio must be pre-encoded as codec tokens (nano_layer_1 through nano_layer_4)\n",
        "- The `encoded_len` column should contain frame counts or duration metadata\n",
        "- Duration filtering happens AFTER merging but BEFORE training\n",
        "- Speaker IDs are converted to lowercase automatically during training\n",
        "- Random sampling (max_len) uses a fixed seed for reproducibility\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CategoricalFilter:\n",
        "    column_name: str = \"speaker\"\n",
        "    value: str = \"ex02\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HFDataset1:\n",
        "    reponame: str = \"nineninesix/expresso-conversational-en-nano-codec-dataset\"\n",
        "    name: Optional[str] = None\n",
        "    split: str = \"train\"\n",
        "    text_col_name: str = \"text\"\n",
        "    nano_layer_1: str = \"nano_layer_1\"\n",
        "    nano_layer_2: str = \"nano_layer_2\"\n",
        "    nano_layer_3: str = \"nano_layer_3\"\n",
        "    nano_layer_4: str = \"nano_layer_4\"\n",
        "    encoded_len: str = \"encoded_len\"\n",
        "    speaker_id: str = \"simon\"\n",
        "    categorical_filter: CategoricalFilter = field(default_factory=CategoricalFilter) # OR None\n",
        "\n",
        "@dataclass\n",
        "class HFDataset2:\n",
        "    reponame: str = \"nineninesix/puck-gemini-flash-en-nano-codec-dataset\"\n",
        "    name: Optional[str] = None\n",
        "    split: str = \"train\"\n",
        "    text_col_name: str = \"text\"\n",
        "    nano_layer_1: str = \"nano_layer_1\"\n",
        "    nano_layer_2: str = \"nano_layer_2\"\n",
        "    nano_layer_3: str = \"nano_layer_3\"\n",
        "    nano_layer_4: str = \"nano_layer_4\"\n",
        "    encoded_len: str = \"encoded_len\"\n",
        "    speaker_id: str = \"puck\"\n",
        "    max_len: int = 2000\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HFDataset3:\n",
        "    reponame: str = \"nineninesix/kore-gemini-flash-en-nano-codec-dataset\"\n",
        "    name: Optional[str] = None\n",
        "    split: str = \"train\"\n",
        "    text_col_name: str = \"text\"\n",
        "    nano_layer_1: str = \"nano_layer_1\"\n",
        "    nano_layer_2: str = \"nano_layer_2\"\n",
        "    nano_layer_3: str = \"nano_layer_3\"\n",
        "    nano_layer_4: str = \"nano_layer_4\"\n",
        "    encoded_len: str = \"encoded_len\"\n",
        "    speaker_id: str = \"kore\"\n",
        "    max_len: int = 2000\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    max_duration_sec: Optional[int] = 12\n",
        "    hf_datasets: List = field(default_factory=lambda: [HFDataset1(),\n",
        "                                                       HFDataset2(),\n",
        "                                                       HFDataset3()])\n"
      ],
      "metadata": {
        "id": "yqeJjZrXzc99",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Processor\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "He who dares peer within shall renounce all understanding, yet in its stead shall find faith!\n",
        "\"\"\"\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from omegaconf import OmegaConf\n",
        "from transformers import AutoTokenizer\n",
        "import locale\n",
        "import os\n",
        "import multiprocessing as mp\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from functools import partial\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TrainDataPreProcessor:\n",
        "    def __init__(self, tokenizer_name: str, max_dur: int, speaker_id: str= None) -> None:\n",
        "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.max_dur = max_dur\n",
        "        self.speaker_id = speaker_id\n",
        "        locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "        self.tokeniser_length = 64400\n",
        "        self.start_of_text = 1\n",
        "        self.end_of_text = 2\n",
        "        self.start_of_speech = self.tokeniser_length + 1\n",
        "        self.end_of_speech = self.tokeniser_length + 2\n",
        "        self.start_of_human = self.tokeniser_length + 3\n",
        "        self.end_of_human = self.tokeniser_length + 4\n",
        "        self.start_of_ai = self.tokeniser_length + 5\n",
        "        self.end_of_ai = self.tokeniser_length + 6\n",
        "        self.pad_token = self.tokeniser_length + 7\n",
        "        self.audio_tokens_start = self.tokeniser_length + 10\n",
        "        self.codebook_size = 4032\n",
        "\n",
        "    def add_codes(self, example) -> list:\n",
        "        snac_layers = ['nano_layer_1', 'nano_layer_2', 'nano_layer_3', 'nano_layer_4']\n",
        "        codes = [example[i] for i in snac_layers]\n",
        "        codes = np.array(codes).T\n",
        "        all_codes = codes + np.array([self.codebook_size * i for i in range(4)])\n",
        "\n",
        "        # remove duplicates\n",
        "        all_codes = self.remove_consecutive_duplicates_np(all_codes)\n",
        "\n",
        "        # flatten to sequence\n",
        "        all_codes = all_codes + self.audio_tokens_start\n",
        "        example[\"codes_list\"] = all_codes.flatten().tolist()\n",
        "        return example\n",
        "\n",
        "    def remove_consecutive_duplicates_np(self, arr: np.ndarray)->np.ndarray:\n",
        "        if arr.ndim != 2:\n",
        "            raise ValueError(\"2D array expected [num_frames, frame_size]\")\n",
        "\n",
        "        mask = np.any(arr[1:] != arr[:-1], axis=1)\n",
        "        keep = np.insert(mask, 0, True)\n",
        "        return arr[keep]\n",
        "\n",
        "\n",
        "    def create_input_ids(self, example):\n",
        "        if self.speaker_id is not None:\n",
        "            text_prompt = f\"{self.speaker_id.lower()}: {example['text']}\"\n",
        "        else:\n",
        "            text_prompt = example[\"text\"]\n",
        "\n",
        "        text_ids = self.text_tokenizer.encode(text_prompt, add_special_tokens=True)\n",
        "        text_ids.append(self.end_of_text)\n",
        "\n",
        "        example[\"text_tokens\"] = text_ids\n",
        "        input_ids = (\n",
        "            [self.start_of_human]\n",
        "            + example[\"text_tokens\"]\n",
        "            + [self.end_of_human]\n",
        "            + [self.start_of_ai]\n",
        "            + [self.start_of_speech]\n",
        "            + example[\"codes_list\"]\n",
        "            + [self.end_of_speech]\n",
        "            + [self.end_of_ai]\n",
        "        )\n",
        "        example[\"input_ids\"] = input_ids\n",
        "        example[\"labels\"] = input_ids\n",
        "        example[\"attention_mask\"] = [1] * len(input_ids)\n",
        "        return example\n",
        "\n",
        "    def __call__(self, dataset: Dataset) -> Dataset:\n",
        "        print(f'üîÑ SHARD PROCESSING: Processing shard with {len(dataset)} samples...')\n",
        "\n",
        "        if self.max_dur:\n",
        "            print(f'üìä FILTER: max duration is -- {self.max_dur} sec --')\n",
        "            dataset_len = len(dataset)\n",
        "            dataset = dataset.filter(lambda i: i['encoded_len']/12.5 <= self.max_dur)\n",
        "            filtred_len = len(dataset)\n",
        "            print(f'‚úÖ COMPLETE {filtred_len} rows from {dataset_len}')\n",
        "\n",
        "        dataset = dataset.map(  self.add_codes,\n",
        "                                remove_columns=['nano_layer_1', 'nano_layer_2', 'nano_layer_3', 'nano_layer_4'],\n",
        "                                desc='Add Audio Codes: ')\n",
        "        dataset = dataset.filter(lambda x: x[\"codes_list\"] is not None, desc='Check codes list')\n",
        "        dataset = dataset.filter(lambda x: len(x[\"codes_list\"]) > 0, desc='Check Codes list lenght')\n",
        "        dataset = dataset.map(self.create_input_ids, remove_columns=[\"text\", \"codes_list\"],\n",
        "                                desc='Create input ids: ')\n",
        "\n",
        "        columns_to_keep = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
        "        columns_to_remove = [col for col in dataset.column_names if col not in columns_to_keep]\n",
        "        dataset = dataset.remove_columns(columns_to_remove)\n",
        "\n",
        "        print(f'‚úÖ SHARD PROCESSING: Completed shard with {len(dataset)} samples')\n",
        "        return dataset\n",
        "\n",
        "\n",
        "def process_shard(shard_idx, shard_data, tokenizer_name, max_dur, speaker_id):\n",
        "    print(f'üöÄ WORKER {shard_idx}: Starting processing...')\n",
        "    processor = TrainDataPreProcessor(tokenizer_name, max_dur, speaker_id)\n",
        "    processed_shard = processor(shard_data)\n",
        "    print(f'‚úÖ WORKER {shard_idx}: Completed processing')\n",
        "    return processed_shard\n",
        "\n",
        "\n",
        "class ItemDataset:\n",
        "    def __init__(self, item_cfg: OmegaConf, tokenizer_name: str, max_dur: int, n_shards: int = None):\n",
        "        print(f'üì¶ DATASET: Loading dataset \"{item_cfg.name}\" from {item_cfg.reponame}...')\n",
        "        self.item_cfg = item_cfg\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.max_dur = max_dur\n",
        "        self.speaker_id = self.item_cfg.get('speaker_id')\n",
        "        self.max_len = self.item_cfg.get('max_len')\n",
        "\n",
        "        if n_shards is None:\n",
        "            self.n_shards = min(mp.cpu_count(), 8)\n",
        "        else:\n",
        "            self.n_shards = n_shards\n",
        "\n",
        "        self.dataset = load_dataset(\n",
        "            self.item_cfg.reponame,\n",
        "            self.item_cfg.name,\n",
        "            split=self.item_cfg.split,\n",
        "            num_proc=10\n",
        "            )\n",
        "\n",
        "        print(f'üìä DATASET: Loaded {len(self.dataset)} samples from {item_cfg.name}')\n",
        "        print(f'üîß DATASET: Will process with {self.n_shards} shards')\n",
        "\n",
        "        if self.item_cfg.get('categorical_filter'):\n",
        "            print(f'üîß DATASET: Filtering by {self.item_cfg.categorical_filter.column_name} = {self.item_cfg.categorical_filter.value}')\n",
        "            self.dataset = self.dataset.filter(lambda x: x[self.item_cfg.categorical_filter.column_name] == self.item_cfg.categorical_filter.value)\n",
        "            print(f'‚úÖ DATASET: Filtered {len(self.dataset)} samples')\n",
        "\n",
        "        print(f'üîÑ DATASET: Renaming columns...')\n",
        "        rename_dict = {\n",
        "            self.item_cfg.text_col_name: 'text',\n",
        "            self.item_cfg.nano_layer_1: 'nano_layer_1',\n",
        "            self.item_cfg.nano_layer_2: 'nano_layer_2',\n",
        "            self.item_cfg.nano_layer_3: 'nano_layer_3',\n",
        "            self.item_cfg.nano_layer_4: 'nano_layer_4',\n",
        "            self.item_cfg.encoded_len: 'encoded_len',\n",
        "        }\n",
        "        self.dataset = self.dataset.rename_columns(rename_dict)\n",
        "        print(f'‚úÖ DATASET: Column renaming completed for {item_cfg.name}')\n",
        "\n",
        "\n",
        "    def __call__(self):\n",
        "        print(f'üîÑ DATASET: Starting parallel processing of {self.item_cfg.name}...')\n",
        "\n",
        "        shards = []\n",
        "        for i in range(self.n_shards):\n",
        "            shard = self.dataset.shard(num_shards=self.n_shards, index=i)\n",
        "            shards.append((shard, i))\n",
        "            print(f'üì¶ SHARD {i}: Created with {len(shard)} samples')\n",
        "\n",
        "        processed_shards = []\n",
        "\n",
        "        with ProcessPoolExecutor(max_workers=self.n_shards) as executor:\n",
        "\n",
        "            future_to_shard = {\n",
        "                executor.submit(process_shard, shard_idx, shard, self.tokenizer_name, self.max_dur, self.speaker_id): shard_idx\n",
        "                for shard, shard_idx in shards\n",
        "            }\n",
        "\n",
        "            for future in as_completed(future_to_shard):\n",
        "                shard_idx = future_to_shard[future]\n",
        "                try:\n",
        "                    processed_shard = future.result()\n",
        "                    processed_shards.append((shard_idx, processed_shard))\n",
        "                    print(f'‚úÖ COMPLETED: Shard {shard_idx} processing finished')\n",
        "                except Exception as exc:\n",
        "                    print(f'‚ùå ERROR: Shard {shard_idx} generated an exception: {exc}')\n",
        "                    raise exc\n",
        "\n",
        "        processed_shards.sort(key=lambda x: x[0])\n",
        "        final_shards = [shard for _, shard in processed_shards]\n",
        "\n",
        "        print(f'üîó DATASET: Concatenating {len(final_shards)} processed shards...')\n",
        "        final_dataset = concatenate_datasets(final_shards)\n",
        "        if self.max_len is not None:\n",
        "            final_dataset = final_dataset.shuffle(seed=42).select(range(self.max_len))\n",
        "        print(f'‚úÖ DATASET: {self.item_cfg.name} processing completed! Final size: {len(final_dataset)} samples')\n",
        "\n",
        "        return final_dataset\n",
        "\n",
        "\n",
        "class DatasetProcessor:\n",
        "    def __init__(self, dataset_config, tokenizer_name: str, n_shards_per_dataset: int = None):\n",
        "        print(f'üöÄ INIT: Initializing DatasetProcessor...')\n",
        "        self.cfg = OmegaConf.structured(dataset_config)\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.n_shards_per_dataset = n_shards_per_dataset\n",
        "        print(f'‚úÖ INIT: DatasetProcessor initialized with {len(self.cfg.hf_datasets)} datasets to process')\n",
        "        if n_shards_per_dataset:\n",
        "            print(f'üîß INIT: Each dataset will be processed with {n_shards_per_dataset} shards')\n",
        "\n",
        "    def __call__(self):\n",
        "        print(f'üîÑ MASTER: Starting master dataset processing...')\n",
        "        datasets = []\n",
        "\n",
        "        for i, item_cfg in enumerate(self.cfg.hf_datasets, 1):\n",
        "            print(f'üì¶ MASTER: Processing dataset {i}/{len(self.cfg.hf_datasets)}: {item_cfg.name}')\n",
        "            item_ds_maker = ItemDataset(\n",
        "                item_cfg=item_cfg,\n",
        "                tokenizer_name=self.tokenizer_name,\n",
        "                max_dur = self.cfg.max_duration_sec,\n",
        "                n_shards=self.n_shards_per_dataset\n",
        "            )\n",
        "            datasets.append(item_ds_maker())\n",
        "\n",
        "        print(f'üîó MASTER: Concatenating all datasets...')\n",
        "        final_dataset = concatenate_datasets(datasets)\n",
        "        final_dataset = final_dataset.shuffle()\n",
        "        print(f'üéâ MASTER: All datasets processed and concatenated! Final dataset size: {len(final_dataset)} samples')\n",
        "        return final_dataset\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "19C4DfPi2WX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_config = Config()\n",
        "dataset_ = DatasetProcessor(dataset_config, MODEL_ID, n_shards_per_dataset=4)\n",
        "train_dataset = dataset_()"
      ],
      "metadata": {
        "id": "HPcLt2F12WQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0h075Rrzc8a",
        "outputId": "d0ce5aed-097c-414b-fdd2-563f5f5372ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'labels', 'attention_mask'],\n",
              "    num_rows: 10604\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUqFiFpe7ZPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Fine-tuning"
      ],
      "metadata": {
        "id": "mnYWSMh07WaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìö Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "print(\"üß† Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"bfloat16\",\n",
        ")"
      ],
      "metadata": {
        "id": "ob_CJzFfzc61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'out_proj', 'w1', 'w2', 'w3', 'in_proj', 'out_proj'],\n",
        "    bias=\"none\",\n",
        "    modules_to_save=None,\n",
        "    use_rslora = True\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "8fF8nUN0zc4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_sft_config = SFTConfig(\n",
        "                            num_train_epochs = 1,\n",
        "                            per_device_train_batch_size = 1,\n",
        "                            gradient_accumulation_steps = 4,\n",
        "                            learning_rate = 5e-5,\n",
        "                            lr_scheduler_type = \"cosine\",\n",
        "                            warmup_ratio = 0.1,\n",
        "                            weight_decay = 0.02,\n",
        "                            optim = \"adamw_torch\",\n",
        "\n",
        "                            overwrite_output_dir=True,\n",
        "                            output_dir=f\"./checkpoints\",\n",
        "                            save_strategy=\"no\",\n",
        "                            remove_unused_columns=True,\n",
        "                            )"
      ],
      "metadata": {
        "id": "rHUwP4LPzc0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üèóÔ∏è  Creating LoRA SFT trainer...\")\n",
        "lora_sft_trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=lora_sft_config,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting LoRA + SFT training...\")\n",
        "lora_sft_trainer.train()\n",
        "\n",
        "print(\"üéâ LoRA + SFT training completed!\")"
      ],
      "metadata": {
        "id": "OnjWgvY7zcye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save merged model\n",
        "\n",
        "Merge the extra weights learned with LoRA back into the model to obtain a \"normal\" model checkpoint."
      ],
      "metadata": {
        "id": "xI1-N-_Ev0cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüîÑ Merging LoRA weights...\")\n",
        "merged_model = lora_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./checkpoints/lora_kani_model_ft_exp\")\n",
        "tokenizer.save_pretrained(\"./checkpoints/lora_kani_model_ft_exp\")\n",
        "print(\"üíæ Merged model saved!\")"
      ],
      "metadata": {
        "id": "_rizEFUsvwce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a945842-e88f-4904-b67f-0de0ca5f5274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Merging LoRA weights...\n",
            "üíæ Merged model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After training, you can publish a model to the Hub:\n",
        "```bash\n",
        "huggingface-cli upload <namespace/repo> ./checkpoints/<model_id> --private\n",
        "```"
      ],
      "metadata": {
        "id": "E4AEqdmwB3Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "If you‚Äôd like to try out the model you‚Äôve just fine-tuned, simply open the inference Colab using the [link](https://colab.research.google.com/drive/1mvzGs7jtAMSUz8wvNlL5uFmgFEyAPjDh?usp=sharing).\n",
        "\n"
      ],
      "metadata": {
        "id": "0qrtqzWOCcMl"
      }
    }
  ]
}