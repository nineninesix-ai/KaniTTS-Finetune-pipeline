base_model: repo/base_model_name
project_name: wandb_project_name

experiments:

  - base:
      model_id: model_name_to_save_local_exp.0.0.1
      run_name: model_name_to_save_local_exp.0.0.1 == run_name
      desc: "description of your experiment"
    lora_args:
      r: 16
      lora_alpha: 16
      lora_dropout: 0.1
      target_modules: [q_proj, w1, w2]
      bias: "none"
      modules_to_save: null
      task_type: CAUSAL_LM
      use_rslora: true
    trainer_args:
      num_train_epochs: 2
      per_device_train_batch_size: 1
      gradient_accumulation_steps: 4
      max_grad_norm: 1
      bf16: true
      learning_rate: 5e-5
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      weight_decay: 0.02
      optim: adamw_torch


  - base:
      model_id: model_name_to_save_local_exp.0.0.2
      run_name: model_name_to_save_local_exp.0.0.2 == run_name
      desc: "description of your experiment"
    lora_args:
      r: 32
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules: [q_proj, k_proj, v_proj, out_proj, w1, w2, w3, in_proj, out_proj]
      bias: "none"
      modules_to_save: null
      task_type: CAUSAL_LM
      use_rslora: true
    trainer_args:
      num_train_epochs: 1
      per_device_train_batch_size: 1
      gradient_accumulation_steps: 4
      max_grad_norm: 1
      bf16: true
      learning_rate: 2e-5
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      weight_decay: 0.02
      optim: adamw_torch
